<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sheng Liu</title>
  
  <meta name="author" content="Sheng Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sheng Liu</name>
              </p>
              <p>
                I am an applied scientist at <a href="https://www.amazon.com/Amazon-Video/b?ie=UTF8&node=2858778011">Amazon Prime Video</a>,
                where I work on computer vision and machine learning. 
              </p>
              <p>
                I obtained my Ph.D. degree from <a href="https://www.buffalo.edu/">University at Buffalo, SUNY</a>, 
                where I was advised by <a href="https://cse.buffalo.edu/~jsyuan/">Prof. Junsong Yuan</a>.
                Before that, I worked at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>. 
                I received my Bachelor's degree from Xian Jiaotong University, 
                where I was a member of the Special Class of the Gifted Young.
              </p>
              <p style="text-align:center">
                <a href="mailto:sliu66@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Sheng Liu CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=2kSyD_EAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sheng-liu-03b69a130/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="data/Sheng Liu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="data/Sheng Liu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li>2023.02: &nbsp; Our paper is accepted by <em><strong>CVPR</strong></em>, 2023.</li>
              <li>2022.05: &nbsp; I joined Amazon Prime Video as an applied scientist.</li>
              <li>2022.05: &nbsp; I graduated &#127891! Thank you Prof. Yuan!</li>
              <li>2022.02: &nbsp; Our paper is accepted by <em><strong>CVPR</strong></em>, 2022.</li>
              <li>2022.10: &nbsp; Our paper is accepted by <em><strong>AAAI</strong></em>, 2022.</li>
            </ul>
            <p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
              At Amazon, I am working on generative AI and 3D scene reconstruction to support <a href="https://advertising.amazon.com/blog/virtual-product-placement">Virtual Product Placement</a>.
              My Ph.D. research focused on vision-and-language (large-scale vision-and-language pre-training in particular), which I am still quite interested. 
              During this time, I also worked on <a href="https://www.matthewtancik.com/nerf">NeRF</a>. 
              We leveraged NeRF to learn kinematic formulas and create 3D avatars.
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Publications</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="lemart_stop()" onmouseover="lemart_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='lemart_image'><video  width=100% height=100% muted autoplay loop>
          <source src="data/lemart.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='images/LEMaRT.png' width="160">
        </div>
        <script type="text/javascript">
          function lemart_start() {
            document.getElementById('lemart_image').style.opacity = "1";
          }

          function lemart_stop() {
            document.getElementById('lemart_image').style.opacity = "0";
          }
          lemart_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <!-- <a href="https://drive.google.com/file/d/1MNRJ5YXGnKMslUjQrIpRXUYjg6pl8trU/view?usp=share_link"> -->
          <papertitle>LEMaRT: Label Efficient Masked Region Transform for Image Harmonization</papertitle>
        <!-- </a> -->
        <br>
        <strong>Sheng Liu</strong>, 
        Cong Phuoc Huynh, Cong Chen, Maxim Arap, Raffay Hamid
        <br>
        <em>CVPR</em>, 2023
        <br>
        <a href="https://drive.google.com/file/d/1MNRJ5YXGnKMslUjQrIpRXUYjg6pl8trU/view?usp=share_link">paper</a>
        <p></p>
        <p>
          We designed a self-supervised pre-training method for image harmonization, 
          which can outperform existing methods 
          while using less than 50% of labeled training data used by them.
        </p>
      </td>
    </tr>
        

    <tr onmouseout="depthsfm_stop()" onmouseover="depthsfm_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='depthsfm_image'>
            <video  width=100% height=100% muted autoplay loop>
            <source src="data/SfM540p.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video>
          </div>
          <img src='images/SfM.png' width="160">
        </div>
        <script type="text/javascript">
          function depthsfm_start() {
            document.getElementById('depthsfm_image').style.opacity = "1";
          }

          function depthsfm_stop() {
            document.getElementById('depthsfm_image').style.opacity = "0";
          }
          depthsfm_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <!-- <a href="https://drive.google.com/file/d/1Vj16FkWXAVSNfFlGlNomOGobnsZCJNcr/view?usp=sharing"> -->
          <papertitle>DepthSfM: Depth Guided Sparse Structure from Motion</papertitle>
        <!-- </a> -->
        <br>
        <strong>Sheng Liu</strong>,
        Xiaohan Nie, Raffay Hamid
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://drive.google.com/file/d/1Vj16FkWXAVSNfFlGlNomOGobnsZCJNcr/view?usp=sharing">paper</a>
        /
        <a href="https://www.youtube.com/watch?v=Sd3ej0DU4bs">video</a>
        /
        <a href="data/depthsfm-poster.pdf">poster</a>
        /
        <a href="https://github.com/amazon-research/small-baseline-camera-tracking">data</a>
        <p></p>
        <p>
        Leveraging depth prior enables us to faithfully reconstruct 3D scene structures from movies and TV shows (and Internet photo collections as well). 
        DepthSfM can reconstruct parts of Los Angeles from a 2-second clip of <a href="https://en.wikipedia.org/wiki/Bosch_(TV_series)">Bosch</a>. 
        Check our <a href="https://www.youtube.com/watch?v=Sd3ej0DU4bs">video</a> to see the results. 
        </p>
      </td>
    </tr>
    
		
    <tr onmouseout="ovis_stop()" onmouseover="ovis_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='ovis_image'>
            <img src='images/ovis.png' width="160"></div> -->
          <img src='images/OVIS.png' width="160">
        </div>
        <script type="text/javascript">
          function ovis_start() {
            document.getElementById('ovis_image').style.opacity = "1";
          }

          function ovis_stop() {
            document.getElementById('ovis_image').style.opacity = "0";
          }
          ovis_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <!-- <a href=""> -->
          <papertitle>OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Pre-training</papertitle>
        <!-- </a> -->
        <br>
        <strong>Sheng Liu</strong>,
        Kevin Lin, Lijuan Wang, Junsong Yuan, Zicheng Liu
        <br>
        <em>AAAI</em>, 2022
        <br>
        <a href="https://drive.google.com/file/d/1Rcotum7eTMAGLMMLBsZ_IlxP3_JsiRh9/view?usp=share_link">paper</a>
        /
        <a href="https://www.youtube.com/watch?v=rqnUZRmLd0g">video</a>
        /
        <a href="data/ovis_data.txt">data</a>
        <p></p>
        <p>
        Large-scale vision-and-language pre-training enables our model to search for and localize visual instances with arbitrary textual queries. 
        Check our 1-minute demo <a href="https://www.youtube.com/watch?v=rqnUZRmLd0g">video</a> in which we compare our method with <a href="https://images.google.com/">Google</a> and <a href="https://www.bing.com/">Bing</a>
        (our method onlys use visual information of images, while Google and Bing leverage textual metadata as well).
        It's interesting &#x1F61C.
        </p>
      </td>
    </tr>


    <tr onmouseout="form_stop()" onmouseover="form_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/Kinematic.png' width="160">
        </div>
        <script type="text/javascript">
          function form_start() {
            document.getElementById('form_image').style.opacity = "1";
          }

          function form_stop() {
            document.getElementById('form_image').style.opacity = "0";
          }
          form_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <!-- <a href=""> -->
          <papertitle>Learning Kinematic Formulas from Multiple View Videos</papertitle>
        <!-- </a> -->
        <br>
        Liangchen Song*
        <strong>Sheng Liu*</strong>,
        Celong Liu, Zhong Li, Yuqi Ding, Yi Xu, Junsong Yuan
        <br>
        <em>ACM MM</em>, 2022
        <br>
        * indicates equal contribution
        <br>
        <a href="https://drive.google.com/file/d/1A7q8_CbNKFPXy10f9Mp7Q_NohV00G6WF/view?usp=share_link">paper</a>
        <p>
        Learning kinematic formulas, <em>e.g.</em>, 
        kinematic equations for objects in free fall, 
        with <a href="https://www.matthewtancik.com/nerf">Neural Radiance Field (NeRF)</a> in an unsupervised manner.        </p>
      </td>
    </tr>


    <tr onmouseout="sibnet_pami_stop()" onmouseover="sibnet_pami_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='sibnet_image'>
            <img src='images/sibnet.png' width="160"></div> -->
          <img src='images/SibNet-PAMI.png' width="160">
        </div>
        <script type="text/javascript">
          function sibnet_start() {
            document.getElementById('sibnet_image').style.opacity = "1";
          }

          function sibnet_pami_stop() {
            document.getElementById('sibnet_image').style.opacity = "0";
          }
          sibnet_pami_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <!-- <a href=""> -->
          <papertitle>SibNet: Sibling Convolutional Encoder for Video Captioning</papertitle>
        <!-- </a> -->
        <br>
        <strong>Sheng Liu</strong>,
        Zhou Ren, Junsong Yuan
        <br>
        <em>TPAMI</em>, 2020
        <br>
        <a href="https://drive.google.com/file/d/1pk7wX5lNeYeObhecfz_eZwCLLMFvYC9B/view?usp=share_link">paper</a>
        <p></p>
        <p>
         Journal version of our ACM MM'18 paper. 
         We designed a two-branch visual encoder for video captioning, 
         with one branch encoding high-level semantic information 
         and the other encoding low-level content information of videos.
        </p>
      </td>
    </tr>

    <tr onmouseout="sibnet_stop()" onmouseover="sibnet_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/SibNet.png' width="160">
        </div>
        <script type="text/javascript">
          function sibnet_start() {
            document.getElementById('sibnet_image').style.opacity = "1";
          }

          function sibnet_stop() {
            document.getElementById('sibnet_image').style.opacity = "0";
          }
          sibnet_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <!-- <a href=""> -->
          <papertitle>SibNet: Sibling Convolutional Encoder for Video Captioning</papertitle>
        <!-- </a> -->
        <br>
        <strong>Sheng Liu</strong>,
        Zhou Ren, Junsong Yuan
        <br>
        <em>ACM MM</em>, 2018 <FONT COLOR="#ff0000"><strong>(Oral)</strong></FONT>
        <br>
        <a href="https://drive.google.com/file/d/1OGgjHCOlNFGRwyYvD7Fj5HKKMqcOBsm3/view?usp=share_link">paper</a>
        <p></p>
        <p>
          We designed a two-branch visual encoder for video captioning, 
          with one branch encoding high-level semantic information 
          and the other encoding low-level content information of videos.
        </p>
      </td>
    </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
				
        <ul>
          <li>Conference reviewer: NeurIPS, ICLR, CVPR, ECCV, ICCV, AAAI</li>
          <li>Journal reviewer: TPAMI, TIP, TCSVT</li>
        </ul>
        
      </td>
    </tr>
  </table>
</body>

</html>
